{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d9e8f3c610a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:58:27.025564Z",
     "start_time": "2024-08-14T08:58:22.337153Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, T5EncoderModel, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f777e769117ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model Architecture and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:11.591477Z",
     "start_time": "2024-08-14T08:58:27.025445Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoavjavits/miniconda3/envs/HSC/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained('google/byt5-base')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9886bc65c6f7ec6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:11.592560Z",
     "start_time": "2024-08-14T08:59:11.591311Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(384, 1536)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(384, 1536)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-17): 17 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(384, 1536)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=384, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8523db035b8c6c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:11.603447Z",
     "start_time": "2024-08-14T08:59:11.591603Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"google/byt5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3968,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1536,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 18,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"ByT5Tokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 384\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7eb01f908bbd84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:11.604250Z",
     "start_time": "2024-08-14T08:59:11.601099Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ByT5Tokenizer(name_or_path='google/byt5-base', vocab_size=256, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>', '<extra_id_100>', '<extra_id_101>', '<extra_id_102>', '<extra_id_103>', '<extra_id_104>', '<extra_id_105>', '<extra_id_106>', '<extra_id_107>', '<extra_id_108>', '<extra_id_109>', '<extra_id_110>', '<extra_id_111>', '<extra_id_112>', '<extra_id_113>', '<extra_id_114>', '<extra_id_115>', '<extra_id_116>', '<extra_id_117>', '<extra_id_118>', '<extra_id_119>', '<extra_id_120>', '<extra_id_121>', '<extra_id_122>', '<extra_id_123>', '<extra_id_124>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t259: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t260: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t261: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t262: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t263: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t264: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t265: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t266: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t267: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t268: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t269: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t270: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t271: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t272: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t273: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t274: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t275: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t276: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t277: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t278: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t279: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t280: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t281: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t282: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t283: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t284: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t285: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t286: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t287: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t288: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t289: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t290: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t291: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t292: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t293: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t294: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t295: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t296: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t297: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t298: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t299: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t300: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t301: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t302: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t303: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t304: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t305: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t306: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t307: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t308: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t309: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t310: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t311: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t312: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t313: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t314: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t315: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t316: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t317: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t318: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t319: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t320: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t321: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t322: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t323: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t324: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t325: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t326: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t327: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t328: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t329: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t330: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t331: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t332: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t333: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t334: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t335: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t336: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t337: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t338: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t339: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t340: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t341: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t342: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t343: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t344: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t345: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t346: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t347: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t348: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t349: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t350: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t351: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t352: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t353: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t354: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t355: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t356: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t357: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t358: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t359: AddedToken(\"<extra_id_100>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t360: AddedToken(\"<extra_id_101>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t361: AddedToken(\"<extra_id_102>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t362: AddedToken(\"<extra_id_103>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t363: AddedToken(\"<extra_id_104>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t364: AddedToken(\"<extra_id_105>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t365: AddedToken(\"<extra_id_106>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t366: AddedToken(\"<extra_id_107>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t367: AddedToken(\"<extra_id_108>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t368: AddedToken(\"<extra_id_109>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t369: AddedToken(\"<extra_id_110>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t370: AddedToken(\"<extra_id_111>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t371: AddedToken(\"<extra_id_112>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t372: AddedToken(\"<extra_id_113>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t373: AddedToken(\"<extra_id_114>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t374: AddedToken(\"<extra_id_115>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t375: AddedToken(\"<extra_id_116>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t376: AddedToken(\"<extra_id_117>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t377: AddedToken(\"<extra_id_118>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t378: AddedToken(\"<extra_id_119>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t379: AddedToken(\"<extra_id_120>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t380: AddedToken(\"<extra_id_121>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t381: AddedToken(\"<extra_id_122>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t382: AddedToken(\"<extra_id_123>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t383: AddedToken(\"<extra_id_124>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b42ce485835294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:11.607248Z",
     "start_time": "2024-08-14T08:59:11.601376Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Stack(\n",
      "  (embed_tokens): Embedding(384, 1536)\n",
      "  (block): ModuleList(\n",
      "    (0): T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "            (relative_attention_bias): Embedding(32, 12)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedActDense(\n",
      "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1-17): 17 x T5Block(\n",
      "      (layer): ModuleList(\n",
      "        (0): T5LayerSelfAttention(\n",
      "          (SelfAttention): T5Attention(\n",
      "            (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (1): T5LayerFF(\n",
      "          (DenseReluDense): T5DenseGatedActDense(\n",
      "            (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "            (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "            (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (act): NewGELUActivation()\n",
      "          )\n",
      "          (layer_norm): T5LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer_norm): T5LayerNorm()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61b64d95b642ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:14.480950Z",
     "start_time": "2024-08-14T08:59:11.605991Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5EncoderModel(\n",
       "  (shared): Embedding(384, 1536)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(384, 1536)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-17): 17 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
       "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_encoder = T5EncoderModel.from_pretrained('google/byt5-base')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5884dc90c878aa2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:14.481589Z",
     "start_time": "2024-08-14T08:59:14.480699Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5EncoderModel(\n",
      "  (shared): Embedding(384, 1536)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(384, 1536)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-17): 17 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=1536, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=1536, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wi_1): Linear(in_features=1536, out_features=3968, bias=False)\n",
      "              (wo): Linear(in_features=3968, out_features=1536, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a937215dc8a8004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:14.481843Z",
     "start_time": "2024-08-14T08:59:14.480802Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"_name_or_path\": \"google/byt5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 3968,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 1536,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 18,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"ByT5Tokenizer\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 384\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(model_encoder.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe004b5898f7dbb9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Check forward pass and hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f9b6f290ed2e78a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:29.076424Z",
     "start_time": "2024-08-14T08:59:29.064667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first option\n",
    "input_ids_first = torch.tensor([list(\"Life is like a box of chocolates.\".encode(\"utf-8\"))]) + 3  # add 3 for special tokens\n",
    "\n",
    "# second option\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_inputs = tokenizer([\"Life is like a box of chocolates.\", \"Today is Monday.\"], padding=\"longest\", return_tensors=\"pt\")\n",
    "input_ids_second = model_inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0246b9bc81e0c97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:30.147570Z",
     "start_time": "2024-08-14T08:59:30.132320Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_first: tensor([[ 79, 108, 105, 104,  35, 108, 118,  35, 111, 108, 110, 104,  35, 100,\n",
      "          35, 101, 114, 123,  35, 114, 105,  35, 102, 107, 114, 102, 114, 111,\n",
      "         100, 119, 104, 118,  49]]) shape: torch.Size([1, 33])\n",
      "input_ids_second: tensor([[ 79, 108, 105, 104,  35, 108, 118,  35, 111, 108, 110, 104,  35, 100,\n",
      "          35, 101, 114, 123,  35, 114, 105,  35, 102, 107, 114, 102, 114, 111,\n",
      "         100, 119, 104, 118,  49,   1],\n",
      "        [ 87, 114, 103, 100, 124,  35, 108, 118,  35,  80, 114, 113, 103, 100,\n",
      "         124,  49,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0]]) shape: torch.Size([2, 34])\n"
     ]
    }
   ],
   "source": [
    "print('input_ids_first:', input_ids_first, 'shape:', input_ids_first.shape)\n",
    "print('input_ids_second:', input_ids_second, 'shape:', input_ids_second.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2233277e27cfb5d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T08:59:54.055836Z",
     "start_time": "2024-08-14T08:59:52.657197Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoavjavits/miniconda3/envs/HSC/lib/python3.8/site-packages/transformers/generation/utils.py:1133: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs are brown, fr\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "input_ids = tokenizer(\"summarize: this dog is brown, friendly and nice\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "925e3b7d4ae11d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T09:00:02.970243Z",
     "start_time": "2024-08-14T09:00:02.708680Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[ 87, 107, 108, 118,  35, 108, 118,  35, 100,  35, 118, 104, 116, 120,\n",
      "         104, 113, 102, 104,  35, 114, 105,  35, 102, 107, 100, 117, 100, 102,\n",
      "         119, 104, 117, 118,  49,   1]], device='cuda:0')\n",
      "input_ids shape: torch.Size([1, 34])\n",
      "encoder_outputs: BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-3.1714e-02, -3.4912e-02,  2.7887e-02,  ..., -2.5939e-02,\n",
      "          -8.8362e-04, -1.3063e-02],\n",
      "         [-7.7431e-02,  4.4391e-02, -3.4005e-02,  ..., -6.0181e-03,\n",
      "          -8.0622e-04, -4.7700e-03],\n",
      "         [-5.6287e-02,  4.2060e-02, -1.3474e-02,  ..., -6.8940e-03,\n",
      "          -8.7906e-04,  5.3880e-02],\n",
      "         ...,\n",
      "         [ 2.3268e-03, -4.1163e-03,  4.8101e-04,  ..., -1.0362e-02,\n",
      "          -6.8147e-04, -1.0306e-02],\n",
      "         [-1.3536e-03, -1.2030e-02, -1.6738e-02,  ...,  4.5562e-02,\n",
      "          -7.5500e-04,  2.7465e-02],\n",
      "         [-2.2792e-04, -2.1033e-03, -4.3789e-03,  ...,  3.7608e-04,\n",
      "          -5.9590e-05, -3.9681e-03]]], device='cuda:0'), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "Last hidden state: tensor([[[-3.1714e-02, -3.4912e-02,  2.7887e-02,  ..., -2.5939e-02,\n",
      "          -8.8362e-04, -1.3063e-02],\n",
      "         [-7.7431e-02,  4.4391e-02, -3.4005e-02,  ..., -6.0181e-03,\n",
      "          -8.0622e-04, -4.7700e-03],\n",
      "         [-5.6287e-02,  4.2060e-02, -1.3474e-02,  ..., -6.8940e-03,\n",
      "          -8.7906e-04,  5.3880e-02],\n",
      "         ...,\n",
      "         [ 2.3268e-03, -4.1163e-03,  4.8101e-04,  ..., -1.0362e-02,\n",
      "          -6.8147e-04, -1.0306e-02],\n",
      "         [-1.3536e-03, -1.2030e-02, -1.6738e-02,  ...,  4.5562e-02,\n",
      "          -7.5500e-04,  2.7465e-02],\n",
      "         [-2.2792e-04, -2.1033e-03, -4.3789e-03,  ...,  3.7608e-04,\n",
      "          -5.9590e-05, -3.9681e-03]]], device='cuda:0')\n",
      "Last hidden state shape: torch.Size([1, 34, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Input sequence of characters\n",
    "input_text = \"This is a sequence of characters.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "print('input_ids:', input_ids)\n",
    "print('input_ids shape:', input_ids.shape)\n",
    "\n",
    "# Pass the input through the encoder to get the hidden states\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    encoder_outputs = model.encoder(input_ids=input_ids)\n",
    "\n",
    "print('encoder_outputs:', encoder_outputs)\n",
    "\n",
    "# Extract the last hidden state (shape: [batch_size, sequence_length, hidden_size])\n",
    "last_hidden_state = encoder_outputs.last_hidden_state\n",
    "\n",
    "# last_hidden_state can now be used as input to another model\n",
    "print(\"Last hidden state:\", last_hidden_state)\n",
    "print(\"Last hidden state shape:\", last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c8f624a844ee77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T09:01:11.703477Z",
     "start_time": "2024-08-14T09:01:11.665930Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs: BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-3.1714e-02, -3.4912e-02,  2.7887e-02,  ..., -2.5939e-02,\n",
      "          -8.8362e-04, -1.3063e-02],\n",
      "         [-7.7431e-02,  4.4391e-02, -3.4005e-02,  ..., -6.0181e-03,\n",
      "          -8.0622e-04, -4.7700e-03],\n",
      "         [-5.6287e-02,  4.2060e-02, -1.3474e-02,  ..., -6.8940e-03,\n",
      "          -8.7906e-04,  5.3880e-02],\n",
      "         ...,\n",
      "         [ 2.3268e-03, -4.1163e-03,  4.8101e-04,  ..., -1.0362e-02,\n",
      "          -6.8147e-04, -1.0306e-02],\n",
      "         [-1.3536e-03, -1.2030e-02, -1.6738e-02,  ...,  4.5562e-02,\n",
      "          -7.5500e-04,  2.7465e-02],\n",
      "         [-2.2792e-04, -2.1033e-03, -4.3789e-03,  ...,  3.7608e-04,\n",
      "          -5.9590e-05, -3.9681e-03]]], device='cuda:0'), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "encoder outputs keys: last_hidden_state\n",
      "Last hidden state: tensor([[[-3.1714e-02, -3.4912e-02,  2.7887e-02,  ..., -2.5939e-02,\n",
      "          -8.8362e-04, -1.3063e-02],\n",
      "         [-7.7431e-02,  4.4391e-02, -3.4005e-02,  ..., -6.0181e-03,\n",
      "          -8.0622e-04, -4.7700e-03],\n",
      "         [-5.6287e-02,  4.2060e-02, -1.3474e-02,  ..., -6.8940e-03,\n",
      "          -8.7906e-04,  5.3880e-02],\n",
      "         ...,\n",
      "         [ 2.3268e-03, -4.1163e-03,  4.8101e-04,  ..., -1.0362e-02,\n",
      "          -6.8147e-04, -1.0306e-02],\n",
      "         [-1.3536e-03, -1.2030e-02, -1.6738e-02,  ...,  4.5562e-02,\n",
      "          -7.5500e-04,  2.7465e-02],\n",
      "         [-2.2792e-04, -2.1033e-03, -4.3789e-03,  ...,  3.7608e-04,\n",
      "          -5.9590e-05, -3.9681e-03]]], device='cuda:0')\n",
      "Last hidden state shape: torch.Size([1, 34, 1536])\n"
     ]
    }
   ],
   "source": [
    "# Pass the input through the encoder to get the hidden states\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    encoder_outputs_encoder_model = model_encoder(input_ids=input_ids)\n",
    "\n",
    "print('encoder_outputs:', encoder_outputs_encoder_model)\n",
    "print('encoder outputs keys:', ' '.join(encoder_outputs_encoder_model.keys()))\n",
    "\n",
    "# Extract the last hidden state (shape: [batch_size, sequence_length, hidden_size])\n",
    "last_hidden_state_encoder_model = encoder_outputs_encoder_model.last_hidden_state\n",
    "\n",
    "# last_hidden_state can now be used as input to another model\n",
    "print(\"Last hidden state:\", last_hidden_state_encoder_model)\n",
    "print(\"Last hidden state shape:\", last_hidden_state_encoder_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d7854f77b1e945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T09:01:12.502616Z",
     "start_time": "2024-08-14T09:01:12.490258Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the hidden states are the same\n",
    "torch.allclose(last_hidden_state, last_hidden_state_encoder_model, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "id": "dda9adcaeeb38804",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "996a189c58de996c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use encoder + LM head for token classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267de1b4901fc010",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:38:29.842886Z",
     "start_time": "2024-08-14T12:38:29.840749Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5EncoderModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c591e867c41cb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:38:31.374760Z",
     "start_time": "2024-08-14T12:38:31.170259Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, block_size):\n",
    "        \"\"\"\n",
    "        file_path: Path to the text file (e.g., 'train.txt')\n",
    "        tokenizer: The ByT5 tokenizer\n",
    "        block_size: The length of the input sequences\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids.squeeze(0)\n",
    "        self.data = tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Input sequence\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        # Target is the next character in the sequence\n",
    "        y = self.data[idx + self.block_size]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d23041a26691ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:38:31.375491Z",
     "start_time": "2024-08-14T12:38:31.371412Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ByT5ForNextCharPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ByT5ForNextCharPrediction, self).__init__()\n",
    "        self.encoder = T5EncoderModel.from_pretrained('google/byt5-base')\n",
    "        self.head = nn.Linear(self.encoder.config.d_model, self.encoder.config.vocab_size)  # Linear layer on top\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        encoder_outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = encoder_outputs.last_hidden_state  # [batch_size, seq_len, hidden_dim]\n",
    "        # We only care about the output of the last token in the sequence\n",
    "        last_hidden_state = sequence_output[:, -1, :]  # [batch_size, hidden_dim]\n",
    "        logits = self.head(last_hidden_state)  # [batch_size, vocab_size]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646d47724b758547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:38:31.397806Z",
     "start_time": "2024-08-14T12:38:31.387637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model_, dataloader, optimizer_, criterion_, device_, epoch):\n",
    "    model_.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch_idx, (input_ids, targets) in enumerate(dataloader):\n",
    "        input_ids = input_ids.to(device_)\n",
    "        targets = targets.to(device_)\n",
    "\n",
    "        optimizer_.zero_grad()\n",
    "        logits = model_(input_ids)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion_(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer_.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Compute the accuracy\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct = (predictions == targets).sum().item()\n",
    "        total_correct += correct\n",
    "        total_predictions += len(targets.view(-1))\n",
    "        \n",
    "        print(total_correct)\n",
    "        print(total_predictions)\n",
    "        \n",
    "\n",
    "        # Logging\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_loss = total_loss / (batch_idx + 1)\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx +1}/{len(dataloader)}], Loss: {avg_loss:.4f}, Accuracy: {total_correct / total_predictions:.4f}')\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_epoch_loss:.4f}')\n",
    "    return avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee261430270dbb92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:38:31.920089Z",
     "start_time": "2024-08-14T12:38:31.912092Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(model_, dataloader, criterion_, device_):\n",
    "    model_.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, targets in dataloader:\n",
    "\n",
    "            # Move data to device        \n",
    "            input_ids = input_ids.to(device_)\n",
    "            targets = targets.to(device_)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model_(input_ids)\n",
    "\n",
    "            loss = criterion_(outputs.logits.view(-1, outputs.logits.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            _, predictions = torch.max(outputs.logits, -1)\n",
    "            total_correct += (predictions == targets).sum().item()\n",
    "            total_predictions += len(targets.view(-1))\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'====> Evaluation Average loss: {avg_loss:.4f}, Accuracy: {total_correct / total_predictions:.4f}')\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a3cf69d59d6d023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T12:38:40.478955Z",
     "start_time": "2024-08-14T12:38:32.622571Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "0\n",
      "8\n",
      "0\n",
      "16\n",
      "1\n",
      "24\n",
      "1\n",
      "32\n",
      "1\n",
      "40\n",
      "1\n",
      "48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion, device)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_, dataloader, optimizer_, criterion_, device_, epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer_\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Compute the accuracy\u001b[39;00m\n\u001b[1;32m     23\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-base')\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = TextDataset('./data/sentences/train.txt', tokenizer, block_size)\n",
    "valid_dataset = TextDataset('./data/sentences/valid.txt', tokenizer, block_size)\n",
    "test_dataset  = TextDataset('./data/sentences/test.txt', tokenizer, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = ByT5ForNextCharPrediction()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'byt5_next_char_prediction.pt')\n",
    "\n",
    "# Test the model\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "616c26d3c0560f55",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
